# -*- coding: utf-8 -*-
"""Sankalpa_A7

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vE5QaHPHAvrBvDzuXf91XjVLTJL45vkw
"""

import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score
import matplotlib.pyplot as plt
import numpy as np

def dbscan_clustering(file_path, eps=1.5, min_samples=5, n_components=2):
    """
    Performs DBSCAN clustering on a dataset from a CSV file with optional dimensionality reduction.

    Args:
        file_path (str): The path to the CSV file.
        eps (float): The maximum distance between two samples for one to be considered as in the neighborhood of the other.
        min_samples (int): The number of samples in a neighborhood for a point to be considered as a core point.
        n_components (int): The number of components for PCA to reduce dimensions (default is 2 for 2D visualization).
    """
    try:
        # Load the dataset
        data = pd.read_csv(file_path)

        print("Dataset preview:")
        print(data.head())

        # Select only numeric columns
        data = data.select_dtypes(include=['float64', 'int64'])

        # Standardize the data
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data)

        # Apply PCA for dimensionality reduction (optional but helpful for visualization)
        pca = PCA(n_components=n_components)
        reduced_data = pca.fit_transform(scaled_data)

        # Apply DBSCAN clustering
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        cluster_labels = dbscan.fit_predict(reduced_data)

        # Print cluster label info
        unique_labels = set(cluster_labels)
        print("Unique Cluster Labels:", unique_labels)
        print("Cluster Labels:", cluster_labels)

        # Count noise points
        noise_count = np.sum(cluster_labels == -1)
        print(f"Number of noise points: {noise_count}")

        # Evaluate clustering performance (ignoring noise)
        core_mask = cluster_labels != -1
        if len(set(cluster_labels[core_mask])) > 1:
            silhouette = silhouette_score(reduced_data[core_mask], cluster_labels[core_mask])
            db_index = davies_bouldin_score(reduced_data[core_mask], cluster_labels[core_mask])
            print(f"Silhouette Score: {silhouette:.4f}")
            print(f"Davies-Bouldin Index: {db_index:.4f}")
        else:
            print("Not enough clusters (excluding noise) to calculate silhouette or Davies-Bouldin Index.")

        # Optional: Visualization
        if n_components == 2:
            plt.figure(figsize=(8, 6))
            plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels, cmap='viridis', s=50)
            plt.title("DBSCAN Clustering")
            plt.xlabel("PCA Component 1")
            plt.ylabel("PCA Component 2")
            plt.colorbar(label='Cluster Label')
            plt.show()

        return cluster_labels

    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
    except Exception as e:
        print(f"An error occurred: {e}")

# Example usage:
file_path = "/content/sample_data/housing.csv"
dbscan_clustering(file_path, eps=1.5, min_samples=5, n_components=2)